{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Object Detection (Tensorflow Model Maker)\n",
    "*This notebook was created based on [Tensorflow example](https://www.tensorflow.org/lite/tutorials/model_maker_object_detection)*\n",
    "\n",
    "The [Model Maker library](https://www.tensorflow.org/lite/guide/model_maker) uses transfer learning to simplify the process of training a TensorFlow Lite model using a custom dataset. Retraining a TensorFlow Lite model with your own custom dataset reduces the amount of training data required and will shorten the training time.\n",
    "\n",
    "**Transfer Learning**\n",
    "Transfer learning is a machine learning technique that enables data scientists to benefit from the knowledge gained from a previously used machine learning model for a similar task. This learning takes humansâ€™ ability to transfer their knowledge as an example. \n",
    "Imagine than you learn how to drive car, you can learn how to drive a truck more easily. \n",
    "Similarly, a model trained for detecting fruits in a image can be use to detect hand gestures.\n",
    "\n",
    "## Setup colab Working directory (Google Colab only)\n",
    "\n",
    "After uploading the *ml-usecase-tensorflow-object-detection* file to your Google Drive and open this notebook you need to mount the drive and setup a working directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "%cd drive/MyDrive/<'path to the ml-usecase-tensorflow-object-detection file'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Perequirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --use-deprecated=legacy-resolver tflite-model-maker\n",
    "!pip install -q pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Workspace\n",
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'my_ssd_mobnet'  # Name of the Network we are going to use \n",
    "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
    "TF_RECORD_SCRIPT_NAME = 'generate_tfrecord.py'\n",
    "\n",
    "paths = {\n",
    "    'WORKSPACE_PATH': os.path.join('MyFirstTFOD','Tensorflow', 'workspace'),\n",
    "    'SCRIPTS_PATH': os.path.join('MyFirstTFOD','Tensorflow','scripts'),\n",
    "    'APIMODEL_PATH': os.path.join('MyFirstTFOD','Tensorflow','models'),\n",
    "    'ANNOTATION_PATH': os.path.join('MyFirstTFOD','Tensorflow', 'workspace','annotations'),\n",
    "    'IMAGE_PATH': os.path.join('MyFirstTFOD','Tensorflow', 'workspace','images'),\n",
    "    'MODEL_PATH': os.path.join('MyFirstTFOD','Tensorflow', 'workspace','models'),\n",
    "    'PRETRAINED_MODEL_PATH': os.path.join('MyFirstTFOD','Tensorflow', 'workspace','pre-trained-models'),\n",
    "    'CHECKPOINT_PATH': os.path.join('MyFirstTFOD','Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME), \n",
    "    'OUTPUT_PATH': os.path.join('MyFirstTFOD','Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'export'), \n",
    "    'TFJS_PATH':os.path.join('MyFirstTFOD','Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfjsexport'), \n",
    "    'TFLITE_PATH':os.path.join('MyFirstTFOD','Tensorflow', 'workspace','models',CUSTOM_MODEL_NAME, 'tfliteexport'), \n",
    "    'PROTOC_PATH':os.path.join('MyFirstTFOD','Tensorflow','protoc')\n",
    " }\n",
    "\n",
    "files = {\n",
    "    'PIPELINE_CONFIG':os.path.join('MyFirstTFOD','Tensorflow', 'workspace','models', CUSTOM_MODEL_NAME, 'pipeline.config'),\n",
    "    'TF_RECORD_SCRIPT': os.path.join(paths['SCRIPTS_PATH'], TF_RECORD_SCRIPT_NAME), \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Here we are going to use the data that you collected in *Image_Collection* notebook. \n",
    "\n",
    "The model maker library lets you load data in 2 different ways: \n",
    "\n",
    "- **From CSV:** learn more about this format [here](https://cloud.google.com/vision/automl/object-detection/docs/csv-format), this is the faster way to consume data using Model Maker.\n",
    "- **From Pascal Voc:** learn more about this format [here](https://towardsdatascience.com/coco-data-format-for-object-detection-a4c5eaf518c5), this is the output format of the LabelImg Tool (that we used).\n",
    "\n",
    "**To learn more go to [Tensorflow Dataloader](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/object_detector/DataLoader)**\n",
    "\n",
    "```python\n",
    "@classmethod\n",
    "\n",
    "from_pascal_voc(\n",
    "    images_dir: str,\n",
    "    annotations_dir: str,\n",
    "    label_map: Union[List[str], Dict[int, str], str],\n",
    "    annotation_filenames: Optional[Collection[str]] = None,\n",
    "    ignore_difficult_instances: bool = False,\n",
    "    num_shards: int = 100,\n",
    "    max_num_images: Optional[int] = None,\n",
    "    cache_dir: Optional[str] = None,\n",
    "    cache_prefix_filename: Optional[str] = None\n",
    ") -> DetectorDataLoader\n",
    "\n",
    "```\n",
    "**To continue we will look at**\n",
    "\n",
    "![pascal_voc](../../docs/pascal_voc.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Label Map\n",
    "\n",
    "The lable map is what allows the model to know the name of the objects that you whant to clssify. So plaese make sure that you provide the same lables that you gave in LableImg.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_map = ['ThumbsUp','ThumbsDown','Peace','ThankYou']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = os.path.join(paths['IMAGE_PATH'],'train')\n",
    "TEST_PATH = os.path.join(paths['IMAGE_PATH'],'test')\n",
    "\n",
    "train_data = object_detector.DataLoader.from_pascal_voc(TRAIN_PATH,TRAIN_PATH,lable_map)\n",
    "validation_data = object_detector.DataLoader.from_pascal_voc(TEST_PATH,TEST_PATH,lable_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Tensorflow Object Detection Model \n",
    "\n",
    "The Model Maker lybrary has many models avaiable like *Audio*, *Image* and *text classification*, object detection, recomendation and Q&A models. \n",
    "We encorage you to look at the list below and test different models. \n",
    "\n",
    "![models](../../docs/models.png)\n",
    "\n",
    "**For this usecase we are going to use a Object Detection **\n",
    "\n",
    "We will use EfficientDet-Lite2 model. \n",
    "EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n",
    "\n",
    "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
    "\n",
    "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
    "|--------------------|-----------|---------------|----------------------|\n",
    "| EfficientDet-Lite0 | 4.4       | 37            | 25.69%               |\n",
    "| EfficientDet-Lite1 | 5.8       | 49            | 30.55%               |\n",
    "| EfficientDet-Lite2 | 7.2       | 69            | 33.97%               |\n",
    "| EfficientDet-Lite3 | 11.4      | 116           | 37.70%               |\n",
    "| EfficientDet-Lite4 | 19.9      | 260           | 41.96%               |\n",
    "\n",
    "<i> * Size of the integer quantized models. <br/>\n",
    "** Latency measured on Pixel 4 using 4 threads on CPU. <br/>\n",
    "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = model_spec.get('efficientdet_lite2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "* The number of epochs will be set to 1000 `epochs = 1000`, which means it will go through the training dataset 1000 times. You can look at the validation accuracy during training and stop early to avoid overfitting.\n",
    "* Set `batch_size = 7` here so you will see that it takes 2 steps to go through the 14 images in the training dataset.\n",
    "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = object_detector.create(train_data, epochs=1000, model_spec=spec, batch_size=7, train_whole_model=True, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "After training the object detection model using the images in the training dataset, use the remaining 25 images in the test dataset to evaluate how the model performs against new data it has never seen before.\n",
    "\n",
    "As the default batch size is 64, it will take 1 step to go through the 25 images in the test dataset.\n",
    "\n",
    "The evaluation metrics are same as [COCO](https://cocodataset.org/#detection-eval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model as a TensorFlow Lite model\n",
    "\n",
    "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is full integer quantization.\n",
    "\n",
    "The export formats can be one or a list of the following:\n",
    "\n",
    "*   `ExportFormat.TFLITE`\n",
    "*   `ExportFormat.LABEL`\n",
    "*   `ExportFormat.SAVED_MODEL`\n",
    "\n",
    "By default, it exports only the TensorFlow Lite model file containing the model [metadata](https://www.tensorflow.org/lite/convert/metadata) so that you can later use in an on-device ML application. The label file is embedded in metadata.\n",
    "\n",
    "In many on-device ML application, the model size is an important factor. Therefore, it is recommended that you quantize the model to make it smaller and potentially run faster. As for EfficientDet-Lite models, full integer quantization  is used to quantize the model by default. Please refer to [Post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(export_dir=paths['TFLITE_PATH'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also choose to export other files related to the model for better examination. For instance, exporting both the saved model and the label file as follows:\n",
    "\n",
    "```python\n",
    "model.export(export_dir=paths['TFLITE_PATH'], export_format=[ExportFormat.SAVED_MODEL, ExportFormat.LABEL])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize Post-training quantization on the TensorFlow Lite model\n",
    "\n",
    "[Post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) is a conversion technique that can reduce model size and inference latency, while also improving CPU and hardware accelerator inference speed, with a little degradation in model accuracy. Thus, it's widely used to optimize the model.\n",
    "\n",
    "Model Maker library applies a default post-training quantization techique when exporting the model. If you want to customize post-training quantization, Model Maker supports multiple post-training quantization options using [QuantizationConfig](https://www.tensorflow.org/lite/api_docs/python/tflite_model_maker/config/QuantizationConfig) as well. Let's take float16 quantization as an instance. First, define the quantization config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = QuantizationConfig.for_float16()\n",
    "model.export(export_dir=paths['TFLITE_PATH'], tflite_filename='model_fp16.tflite', quantization_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try by yourself\n",
    "\n",
    "### Customize the EfficientDet model hyperparameters\n",
    "\n",
    "The model and training pipline parameters you can adjust are:\n",
    "\n",
    "* `model_dir`: The location to save the model checkpoint files. If not set, a temporary directory will be used.\n",
    "* `steps_per_execution`: Number of steps per training execution.\n",
    "* `moving_average_decay`: Float. The decay to use for maintaining moving averages of the trained parameters.\n",
    "* `var_freeze_expr`: The regular expression to map the prefix name of variables to be frozen which means remaining the same during training. More specific, use `re.match(var_freeze_expr, variable_name)` in the codebase to map the variables to be frozen.\n",
    "* `tflite_max_detections`: integer, 25 by default. The max number of output detections in the TFLite model.\n",
    "* `strategy`:  A string specifying which distribution strategy to use. Accepted values are 'tpu', 'gpus', None. tpu' means to use TPUStrategy. 'gpus' mean to use MirroredStrategy for multi-gpus. If None, use TF default with OneDeviceStrategy.\n",
    "* `tpu`:  The Cloud TPU to use for training. This should be either the name used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.\n",
    "* `use_xla`: Use XLA even if strategy is not tpu. If strategy is tpu, always use XLA, and this flag has no effect.\n",
    "* `profile`: Enable profile mode.\n",
    "* `debug`: Enable debug mode.\n",
    "\n",
    "Other parameters that can be adjusted is shown in [hparams_config.py](https://github.com/google/automl/blob/df451765d467c5ed78bbdfd632810bc1014b123e/efficientdet/hparams_config.py#L170).\n",
    "\n",
    "For instance, you can set the `var_freeze_expr='efficientnet'` which freezes the variables with name prefix `efficientnet` (default is `'(efficientnet|fpn_cells|resample_p6)'`). This allows the model to freeze untrainable variables and keep their value the same through training.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = model_spec.get('efficientdet_lite0')\n",
    "spec.config.var_freeze_expr = 'efficientnet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the Model Architecture\n",
    "\n",
    "You can change the model architecture by changing the `model_spec`. For instance, change the `model_spec` to the EfficientDet-Lite4 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = model_spec.get('efficientdet_lite4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the training hyperparameters\n",
    "\n",
    "The `create` function is the driver function that the Model Maker library uses to create models. The `model_spec` parameter defines the model specification. The `object_detector.EfficientDetSpec` class is currently supported. The `create` function comprises of the following steps:\n",
    "\n",
    "1. Creates the model for the object detection according to `model_spec`.\n",
    "2. Trains the model.  The default epochs and the default batch size are set by the `epochs` and `batch_size` variables in the `model_spec` object.\n",
    "You can also tune the training hyperparameters like `epochs` and `batch_size` that affect the model accuracy. For instance,\n",
    "\n",
    "*   `epochs`: Integer, 50 by default. More epochs could achieve better accuracy, but may lead to overfitting.\n",
    "*   `batch_size`: Integer, 64 by default. The number of samples to use in one training step.\n",
    "*   `train_whole_model`: Boolean, False by default. If true, train the whole model. Otherwise, only train the layers that do not match `var_freeze_expr`.\n",
    "\n",
    "For example, you can train with less epochs and only the head layer. You can increase the number of epochs for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = # TODO: Fill in to create a model with new hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your model in one of the test images\n",
    "\n",
    "**Load Model and make sure Model Path exists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract model tar file (#Colab only)\n",
    "MODEL_FILE = os.path.join('model.tflite')\n",
    "if os.path.exists(MODEL_FILE):\n",
    "    if not os.path.exists(paths['TFLITE_PATH']):\n",
    "        os.makedirs(paths['TFLITE_PATH'])\n",
    "    !mv {MODEL_FILE} {paths['TFLITE_PATH']} \n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=os.path.join(paths['TFLITE_PATH'],'model.tflite'))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['??'] * 4\n",
    "label_map = {1:'ThumbsUp', 2:'ThumbsDown', 3:'Peace', 4:'ThankYou'}\n",
    "for label_id, label_name in label_map.items():\n",
    "  classes[label_id-1] = label_name\n",
    "\n",
    "# Define a list of colors for visualization\n",
    "COLORS = np.random.randint(0, 255, size=(len(classes), 3), dtype=np.uint8)\n",
    "\n",
    "\n",
    "def preprocess_image(image_path, input_size):\n",
    "  \"\"\"Preprocess the input image to feed to the TFLite model\"\"\"\n",
    "  img = tf.io.read_file(image_path)\n",
    "  img = tf.io.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "  original_image = img\n",
    "  resized_img = tf.image.resize(img, input_size)\n",
    "  resized_img = resized_img[tf.newaxis, :]\n",
    "  resized_img = tf.cast(resized_img, dtype=tf.uint8)\n",
    "  return resized_img, original_image\n",
    "\n",
    "\n",
    "def detect_objects(interpreter, image, threshold, k):\n",
    "  \"\"\"Returns a list of detection results, each a dictionary of object info.\"\"\"\n",
    "\n",
    "  signature_fn = interpreter.get_signature_runner()\n",
    "\n",
    "  # Feed the input image to the model\n",
    "  output = signature_fn(images=image)\n",
    "\n",
    "  # Get all outputs from the model\n",
    "  count = int(np.squeeze(output['output_0']))\n",
    "  scores = np.squeeze(output['output_1'])\n",
    "  classes = np.squeeze(output['output_2'])\n",
    "  boxes = np.squeeze(output['output_3'])\n",
    "  \n",
    "  results = []\n",
    "  for i in range(count):\n",
    "    if scores[i] >= threshold:\n",
    "      result = {\n",
    "        'bounding_box': boxes[i],\n",
    "        'class_id': classes[i],\n",
    "        'score': scores[i]\n",
    "      }\n",
    "      results.append(result)\n",
    "  \n",
    "  return results[:k]\n",
    "\n",
    "\n",
    "def run_odt_and_draw_results(image_path, interpreter, threshold=0.5, k=3):\n",
    "  \"\"\"Run object detection on the input image and draw the detection results\"\"\"\n",
    "  # Load the input shape required by the model\n",
    "  _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n",
    "\n",
    "  # Load the input image and preprocess it\n",
    "  preprocessed_image, original_image = preprocess_image(\n",
    "      image_path,\n",
    "      (input_height, input_width)\n",
    "    )\n",
    "\n",
    "  # Run object detection on the input image\n",
    "  results = detect_objects(interpreter, preprocessed_image, threshold=threshold, k=k)\n",
    "\n",
    "  # Plot the detection results on the input image\n",
    "  original_image_np = original_image.numpy().astype(np.uint8)\n",
    "  for obj in results:\n",
    "    # Convert the object bounding box from relative coordinates to absolute\n",
    "    # coordinates based on the original image resolution\n",
    "    ymin, xmin, ymax, xmax = obj['bounding_box']\n",
    "    xmin = int(xmin * original_image_np.shape[1])\n",
    "    xmax = int(xmax * original_image_np.shape[1])\n",
    "    ymin = int(ymin * original_image_np.shape[0])\n",
    "    ymax = int(ymax * original_image_np.shape[0])\n",
    "\n",
    "    # Find the class index of the current object\n",
    "    class_id = int(obj['class_id'])\n",
    "\n",
    "    # Draw the bounding box and label on the image\n",
    "    color = [int(c) for c in COLORS[class_id]]\n",
    "    cv2.rectangle(original_image_np, (xmin, ymin), (xmax, ymax), color, 2)\n",
    "    # Make adjustments to make the label visible for all objects\n",
    "    y = ymin - 15 if ymin - 15 > 15 else ymin + 15\n",
    "    label = \"{}: {:.0f}%\".format(classes[class_id], obj['score'] * 100)\n",
    "    cv2.putText(original_image_np, label, (xmin, y),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "  # Return the final image\n",
    "  original_uint8 = original_image_np.astype(np.uint8)\n",
    "  return original_uint8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open CV\n",
    "\n",
    "In the *TEST_IMG* variable paste the name of one of your test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to images \n",
    "TEST_PATH = os.path.join(paths['IMAGE_PATH'],'test')\n",
    "RESULTS_PATH = os.path.join(paths['IMAGE_PATH'],'results')\n",
    "\n",
    "TEST_IMG = 'peace.401cd57a-a934-11ec-8dc7-dca904818221.jpg' \n",
    "TEST_IMG_PATH = os.path.join(TEST_PATH, TEST_IMG)\n",
    "RESULT_IMG_PATH = os.path.join(RESULTS_PATH,'test_img.jpg')\n",
    "\n",
    "if not os.path.exists(RESULT_IMG_PATH):\n",
    "    os.makedirs(RESULTS_PATH)\n",
    "    !cp {TEST_IMG_PATH} {RESULT_IMG_PATH}\n",
    "\n",
    "im = Image.open(RESULT_IMG_PATH)\n",
    "im.thumbnail((512, 512), Image.ANTIALIAS)\n",
    "im.save(RESULT_IMG_PATH, 'PNG')\n",
    "\n",
    "\n",
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=os.path.join(paths['TFLITE_PATH'],'model.tflite'))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "DETECTION_THRESHOLD = 0.1\n",
    "\n",
    "# Run inference and draw detection result on the local copy of the original file\n",
    "detection_result_image = run_odt_and_draw_results(\n",
    "    RESULT_IMG_PATH,\n",
    "    interpreter,\n",
    "    threshold=DETECTION_THRESHOLD,\n",
    "    k=1\n",
    ")\n",
    "# Show the detection result\n",
    "Image.fromarray(detection_result_image)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "623f254610726c268c56d21a8778f851f03d647c51f2790b03209b9698e47c41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
